{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4df579be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "from torchtext import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a2376764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['PhraseId', 'SentenceId', 'Phrase', 'Sentiment'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dir_all_data = '/Users/apple/Desktop/data/sentiment-analysis-on-movie-reviews/train.tsv'\n",
    "data_all = pd.read_csv(dir_all_data, sep='\\t')\n",
    "print(data_all.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e655b391",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "idx = np.arange(len(data_all))\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "train_size = len(idx * 0.6)\n",
    "test_size = len(idx * 0.8)\n",
    "\n",
    "data_all.iloc[idx[:train_size], :].to_csv('/Users/apple/Desktop/data/RNN_train.csv', index = False)\n",
    "data_all.iloc[idx[train_size:test_size], :].to_csv('/Users/apple/Desktop/data/RNN_test.csv', index = False)\n",
    "data_all.iloc[idx[test_size:], :].to_csv('/Users/apple/Desktop/data/RNN_val.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "40777280",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = '<pad>'\n",
    "\n",
    "TEXT = data.Field(sequential = True, lower = True, batch_first = True, pad_token = PAD_TOKEN)\n",
    "LABEL = data.Field(sequential = False, batch_first = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ee4a8d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datafields = [\n",
    "    (\"PhraseId\", None),\n",
    "    (\"SentenceId\", None),\n",
    "    (\"Phrase\", TEXT),\n",
    "    (\"Sentiment\", LABEL)\n",
    "]\n",
    "\n",
    "train_data = data.TabularDataset(path = '/Users/apple/Desktop/data/RNN_train.csv', format = 'csv', fields = datafields)\n",
    "test_data = data.TabularDataset(path = '/Users/apple/Desktop/data/RNN_test.csv', format = 'csv', fields = datafields)\n",
    "val_data = data.TabularDataset(path = '/Users/apple/Desktop/data/RNN_val.csv', format = 'csv', fields = datafields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "71fe5f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data, vectors = 'glove.6B.50d')\n",
    "LABEL.build_vocab(train_data)\n",
    "PAD_INDEX = TEXT.vocab.stoi[PAD_TOKEN]\n",
    "TEXT.vocab.vectors[PAD_INDEX] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "20d8fb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = data.BucketIterator(train_data, batch_size = BATCH_SIZE, train = True, shuffle = True, device = DEVICE)\n",
    "test_iterator = data.Iterator(test_data, batch_size = BATCH_SIZE, train = False, sort = False, device = DEVICE)\n",
    "val_iterator = data.Iterator(val_data, batch_size = BATCH_SIZE, train = False, sort = False, device = DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "632f09d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_choice = 'glove'\n",
    "embedding_dim = 50\n",
    "num_embeddings = len(TEXT.vocab)\n",
    "word_size = len(TEXT.vocab)\n",
    "label_size = len(LABEL.vocab)\n",
    "hidden_layer = 50\n",
    "num_layers = 2\n",
    "dropout = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "569430cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super (LSTM, self).__init__()\n",
    "        self.embedding_choice = embedding_choice\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim, padding_idx = PAD_INDEX).from_pretrained(TEXT.vocab.vectors, freeze = True)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_layer, num_layers, batch_first = True, dropout=dropout, bidirectional = True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(2 * hidden_layer, label_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        c0 = torch.zeros(num_layers * 2, x.size(0), self.hidden_layer).to(DEVICE)\n",
    "        h0 = torch.zeros(num_layers * 2, x.size(0), self.hidden_layer).to(DEVICE)\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.dropout(out)\n",
    "        out = torch.cat((out[:, 0, self.hidden_layer: ], out[:, -1, :self.hidden_layer]), dim = 1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bc8119d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (embedding): Embedding(16533, 50)\n",
       "  (lstm): LSTM(50, 50, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=100, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTM()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.005)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f31fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0_2.050%: Training average Loss: 0.963284\n",
      "Epoch 0_4.101%: Training average Loss: 0.956799\n",
      "Epoch 0_6.151%: Training average Loss: 0.952459\n",
      "Epoch 0_8.202%: Training average Loss: 0.955679\n",
      "Epoch 0_10.252%: Training average Loss: 0.955541\n",
      "Epoch 0_12.303%: Training average Loss: 0.954844\n",
      "Epoch 0_14.353%: Training average Loss: 0.955034\n",
      "Epoch 0_16.404%: Training average Loss: 0.955254\n",
      "Epoch 0_18.454%: Training average Loss: 0.955624\n",
      "Epoch 0_20.505%: Training average Loss: 0.954579\n",
      "Epoch 0_22.555%: Training average Loss: 0.955337\n",
      "Epoch 0_24.606%: Training average Loss: 0.954604\n",
      "Epoch 0_26.656%: Training average Loss: 0.953264\n",
      "Epoch 0_28.707%: Training average Loss: 0.954883\n",
      "Epoch 0_30.757%: Training average Loss: 0.956191\n",
      "Epoch 0_32.808%: Training average Loss: 0.954977\n",
      "Epoch 0_34.858%: Training average Loss: 0.955198\n",
      "Epoch 0_36.909%: Training average Loss: 0.954853\n",
      "Epoch 0_38.959%: Training average Loss: 0.955667\n",
      "Epoch 0_41.010%: Training average Loss: 0.955096\n",
      "Epoch 0_43.060%: Training average Loss: 0.954977\n",
      "Epoch 0_45.111%: Training average Loss: 0.954345\n",
      "Epoch 0_47.161%: Training average Loss: 0.953859\n",
      "Epoch 0_49.212%: Training average Loss: 0.954295\n",
      "Epoch 0_51.262%: Training average Loss: 0.953678\n",
      "Epoch 0_53.312%: Training average Loss: 0.953179\n",
      "Epoch 0_55.363%: Training average Loss: 0.952843\n",
      "Epoch 0_57.413%: Training average Loss: 0.951790\n",
      "Epoch 0_59.464%: Training average Loss: 0.950980\n",
      "Epoch 0_61.514%: Training average Loss: 0.951190\n",
      "Epoch 0_63.565%: Training average Loss: 0.950938\n",
      "Epoch 0_65.615%: Training average Loss: 0.950749\n",
      "Epoch 0_67.666%: Training average Loss: 0.949997\n",
      "Epoch 0_69.716%: Training average Loss: 0.949801\n",
      "Epoch 0_71.767%: Training average Loss: 0.949465\n",
      "Epoch 0_73.817%: Training average Loss: 0.949487\n",
      "Epoch 0_75.868%: Training average Loss: 0.949214\n",
      "Epoch 0_77.918%: Training average Loss: 0.949769\n",
      "Epoch 0_79.969%: Training average Loss: 0.949535\n",
      "Epoch 0_82.019%: Training average Loss: 0.949146\n",
      "Epoch 0_84.070%: Training average Loss: 0.948291\n",
      "Epoch 0_86.120%: Training average Loss: 0.948064\n",
      "Epoch 0_88.171%: Training average Loss: 0.947945\n",
      "Epoch 0_90.221%: Training average Loss: 0.947824\n",
      "Epoch 0_92.272%: Training average Loss: 0.947943\n",
      "Epoch 0_94.322%: Training average Loss: 0.947850\n",
      "Epoch 0_96.373%: Training average Loss: 0.947304\n"
     ]
    }
   ],
   "source": [
    "#开始训练\n",
    "epoch = 1\n",
    "best_accuracy = 0.0\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(epoch):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    accuracy = 0.0\n",
    "    total_correct = 0.0\n",
    "    total_data_num = len(train_iterator.dataset)\n",
    "    steps = 0.0\n",
    "    for batch in train_iterator:\n",
    "        steps += 1\n",
    "        optimizer.zero_grad() #  梯度缓存清零\n",
    "        batch_text = batch.Phrase\n",
    "        batch_label = batch.Sentiment\n",
    "        out = model(batch_text)    #[batch_size, label_num]\n",
    "        loss = criterion(out, batch_label)\n",
    "        total_loss += loss.item() \n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        correct = (torch.max(out, dim=1)[1] == batch_label).sum()\n",
    "        total_correct += correct.item()\n",
    "        if steps % 100 == 0:\n",
    "            print(\"Epoch %d_%.3f%%: Training average Loss: %f\" \n",
    "                  % (i, steps * train_iterator.batch_size * 100 / len(train_iterator.dataset), total_loss / steps))  \n",
    "    #每个epoch都验证一下\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    accuracy = 0.0\n",
    "    total_correct = 0.0\n",
    "    total_data_num = len(val_iterator.dataset)\n",
    "    steps = 0.0    \n",
    "    for batch in val_iterator:\n",
    "        steps += 1\n",
    "        batch_text = batch.Phrase\n",
    "        batch_label = batch.Sentiment\n",
    "        out = model(batch_text)\n",
    "        loss = criterion(out, batch_label)\n",
    "        total_loss += loss.item()\n",
    "        correct = (torch.max(out, dim=1)[1] == batch_label).sum()\n",
    "        total_correct += correct.item()\n",
    "        print(\"Epoch %d :  Verification average Loss: %f, Verification accuracy: %f%%, Total Time:%f\"\n",
    "          %(i, total_loss / steps, total_correct * 100 / total_data_num, time.time() - start_time))  \n",
    "    if best_accuracy < total_accuracy / total_data_num:\n",
    "        best_accuracy = total_accuracy / total_data_num\n",
    "        torch.save(model, '/Users/apple/desktop/NLP_base/RNN_TEXT_CLASSIFICATION')\n",
    "        print(\"Model is saved %f with accuracy %f\" % (i, total_accuracy / total_data_num))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111353dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
